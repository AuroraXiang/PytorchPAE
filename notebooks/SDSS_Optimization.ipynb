{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1335cfa7-3743-46cc-9005-b8b57450a9cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.8.0\n",
      "cuda:0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[I 2022-01-28 10:24:02,367]\u001b[0m Using an existing study with name '/global/cscratch1/sd/vboehm/OptunaStudies/SDSS_conv_AE_optimization' instead of creating a new one.\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using an existing study with name '/global/cscratch1/sd/vboehm/OptunaStudies/SDSS_conv_AE_optimization' instead of creating a new one.\n",
      "Study statistics: \n",
      "  Number of finished trials:  71\n",
      "  Number of pruned trials:  0\n",
      "  Number of complete trials:  70\n",
      "Best trial:\n",
      "  Value:  1.1444383394722513\n",
      "  Params: \n",
      "    kernel_size_0: 44\n",
      "    latent_dim: 10\n",
      "    layer_norm_0: False\n",
      "    n_layers: 1\n",
      "    out_channel_0: 32\n",
      "    scale_fac_0: 1\n"
     ]
    }
   ],
   "source": [
    "from pytorch_pae import AE, utils\n",
    "import optuna\n",
    "from optuna.trial import TrialState\n",
    "import logging\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import sys\n",
    "\n",
    "import torch\n",
    "from torchsummary import summary\n",
    "print(torch.__version__)\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "\n",
    "optuna.logging.get_logger(\"optuna\").addHandler(logging.StreamHandler(sys.stdout))\n",
    "\n",
    "study_folder  = '/global/cscratch1/sd/vboehm/OptunaStudies/'\n",
    "study_name    = \"SDSS_conv_AE_optimization\"  \n",
    "study_name    = os.path.join(study_folder, study_name)\n",
    "storage_name  = \"sqlite:///{}.db\".format(study_name)\n",
    "EPOCHS        = 15\n",
    "NUM_HOURS     = 4\n",
    "N_TRIALS      = 100\n",
    "SEED          = 314159\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# def objective(trial):\n",
    "    \n",
    "    \n",
    "#     ## data parameters\n",
    "#     dataset       = 'SDSS_DR16'\n",
    "#     loc           = '/global/cscratch1/sd/vboehm/Datasets'\n",
    "\n",
    "#     # padding values in each conv layer\n",
    "#     strides      = [1]\n",
    "#     paddings     = [0]\n",
    "#     # stride values in each conv layer\n",
    "#     # whether tp apply a layer normalization after conv layer\n",
    "#     #layer_norm   = [True]\n",
    "#     # whether to train elemntwise affine parameters for normalization layer \n",
    "#     affine       = False\n",
    "#     # whether to Lipschitz regularize by bounding the spectral norm \n",
    "#     spec_norm    = True\n",
    "#     # activation function after each layer\n",
    "#     activations  = ['ReLU']\n",
    "#     # whether to add a bias in each layer or not\n",
    "#     bias         = [True]\n",
    "\n",
    "#     # data dimensionality\n",
    "#     dim          = '1D'\n",
    "#     # number of channels in data\n",
    "#     input_c      = 1 \n",
    "#     # data dimensioality along one axis (only square data supported in 2D)\n",
    "#     input_dim    = 1000\n",
    "#     # type of encoder and decoder network (either 'fc' or 'conv')\n",
    "#     encoder_type = 'conv'\n",
    "#     decoder_type = 'conv'\n",
    "\n",
    "#     # if True, the output is fed through a sigmoid layer to bring data values into range [0,1]\n",
    "#     final_sigmoid = False\n",
    "\n",
    "#     ## Training parameters\n",
    "#     nepochs       = EPOCHS\n",
    "#     batchsize     = 32\n",
    "#     initial_lr    = 1e-2\n",
    "\n",
    "#     optimizer        = 'Adam'\n",
    "#     criterion1       = 'MSELoss'\n",
    "#     criterion2       = 'masked_chi2'\n",
    "\n",
    "#     scheduler        = 'ExponentialLR'\n",
    "#     scheduler_params = {'gamma':0.95}\n",
    "#     ann_epoch        = 5\n",
    "    \n",
    "#     n_layers   = trial.suggest_int('n_layers',1,6)\n",
    "#     latent_dim = trial.suggest_int('latent_dim',8,12)\n",
    "    \n",
    "#     activations = activations*n_layers\n",
    "#     bias        = bias*n_layers\n",
    "#     #layer_norm  = layer_norm*n_layers\n",
    "#     strides     = strides*n_layers\n",
    "#     paddings    = paddings*n_layers\n",
    "    \n",
    "    \n",
    "#     out_channels = []\n",
    "#     kernel_sizes = []\n",
    "#     scale_facs   = []\n",
    "#     dropout_rate = []\n",
    "#     layer_norm   = []\n",
    "#     #paddings     = []\n",
    "    \n",
    "#     current_size = input_dim \n",
    "#     for ii in range(n_layers):\n",
    "#         out_channels.append(trial.suggest_int('out_channel_%d'%ii,8,128))\n",
    "#         kernel_sizes.append(trial.suggest_int('kernel_size_%d'%ii,3,max(3,int(current_size)//20)))\n",
    "#         #paddings.append(trial.suggest_int('padding_%d'%ii,1,kernel_sizes[ii]//2))\n",
    "#         current_size = utils.output_shape(current_size,strides[ii],paddings[ii],kernel_sizes[ii],dilation=1)\n",
    "#         layer_norm.append(trial.suggest_categorical('layer_norm_%d'%ii,[True,False]))\n",
    "#         if current_size>8*latent_dim:\n",
    "#             scale_facs.append(trial.suggest_int('scale_fac_%d'%ii,1,4))\n",
    "#         else:\n",
    "#             scale_facs.append(1)\n",
    "#         current_size = current_size//scale_facs[ii]\n",
    "            \n",
    "#         #dropout_rate.append(trial.suggest_float('dropout_rate_%d'%ii,1e-3,1,log=True))\n",
    "\n",
    "#     print(kernel_sizes,paddings,strides)\n",
    "    \n",
    "#     general_params      = {'input_c': input_c, 'input_dim': input_dim, 'latent_dim': latent_dim, 'encoder_type': encoder_type, 'decoder_type': decoder_type, 'dim': dim}\n",
    "#     conv_network_params = {'n_layers': n_layers, 'out_channels': out_channels, 'kernel_sizes': kernel_sizes, 'scale_facs': scale_facs, 'paddings': paddings,                       'strides': strides,'activations': activations, 'spec_norm': spec_norm, 'layer_norm': layer_norm,                       'affine': affine,'final_sigmoid': final_sigmoid, 'bias':bias}\n",
    "\n",
    "#     training_params     = {'batchsize': batchsize, 'initial_lr': initial_lr, 'optimizer': optimizer, 'criterion1': criterion1, 'criterion2': criterion2,\n",
    "#                            'scheduler': scheduler, 'scheduler_params':scheduler_params, 'ann_epoch': ann_epoch}\n",
    "#     data_params         = {'dataset':dataset, 'loc': loc}\n",
    "    \n",
    "#     AE1                 = AE.Autoencoder(general_params,data_params,conv_network_params, conv_network_params, training_params, device)\n",
    "\n",
    "#     try:\n",
    "#         if dim =='1D':\n",
    "#             summary(AE1, (input_c,input_dim))\n",
    "#         else:\n",
    "#             summary(AE1, (input_c, input_dim, input_dim))\n",
    "        \n",
    "#         train_loss, valid_loss = AE1.train(nepochs)\n",
    "#     except:\n",
    "#         valid_loss=[100]\n",
    "        \n",
    "    \n",
    "#     return np.sqrt(valid_loss[-1])\n",
    "\n",
    "\n",
    "# In[17]:\n",
    "\n",
    "\n",
    "time = NUM_HOURS*60*60-600\n",
    "study = optuna.create_study(direction='minimize',study_name=study_name, storage=storage_name,load_if_exists=True,  sampler=optuna.samplers.TPESampler(seed=SEED),\n",
    "    pruner=optuna.pruners.MedianPruner(n_warmup_steps=10))\n",
    "#study.optimize(objective, n_trials=N_TRIALS, timeout=time)\n",
    "\n",
    "\n",
    "# In[18]:\n",
    "\n",
    "\n",
    "pruned_trials = study.get_trials(deepcopy=False, states=[TrialState.PRUNED])\n",
    "complete_trials = study.get_trials(deepcopy=False, states=[TrialState.COMPLETE])\n",
    "\n",
    "print(\"Study statistics: \")\n",
    "print(\"  Number of finished trials: \", len(study.trials))\n",
    "print(\"  Number of pruned trials: \", len(pruned_trials))\n",
    "print(\"  Number of complete trials: \", len(complete_trials))\n",
    "\n",
    "print(\"Best trial:\")\n",
    "trial = study.best_trial\n",
    "\n",
    "print(\"  Value: \", trial.value)\n",
    "\n",
    "print(\"  Params: \")\n",
    "for key, value in trial.params.items():\n",
    "    print(\"    {}: {}\".format(key, value))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "25c56176-cf01-4ad3-bf8e-986982189496",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50.0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1000/20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37c2cf02-1e75-4774-9ea6-3368e3afca2e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
