{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e47cd393-a7ab-450d-952f-43c889a538fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "sys.path.append('/global/u2/v/vboehm/codes/SIG_GIS/')\n",
    "from sig_gis import *\n",
    "from sig_gis.GIS import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "39d2a8cd-39a9-4fb7-ae0d-0d3a45c06734",
   "metadata": {},
   "outputs": [],
   "source": [
    "#some fudge data\n",
    "train_x  = np.random.random((100,2))\n",
    "#some fudge labels\n",
    "train_y = np.append(np.ones(50),np.zeros(50))\n",
    "np.random.shuffle(train_y)\n",
    "\n",
    "#some fudge data\n",
    "valid_x  = np.random.random((50,2))\n",
    "#some fudge labels\n",
    "valid_y = np.append(np.ones(25),np.zeros(25))\n",
    "np.random.shuffle(valid_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "372bf240-0fa4-4787-8e25-0c44c114644f",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "96c6bfc3-5923-49db-bb15-b02afa5d2559",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = torch.Tensor(train_x).to(device) \n",
    "train_y = torch.Tensor(train_y).to(torch.long).to(device)\n",
    "\n",
    "valid_x = torch.Tensor(valid_x).to(device) \n",
    "valid_y = torch.Tensor(valid_y).to(torch.long).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7aa62784-22f7-437e-b10d-585a9440991c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _conditional_transform_batch_model(model, data, label, logj, index, batchsize, start_index=0, end_index=None, start=0, end=None, param=None, nocuda=False):\n",
    "\n",
    "    if torch.cuda.is_available() and not nocuda:\n",
    "        gpu    = index % torch.cuda.device_count()\n",
    "        device = torch.device('cuda:%d'%gpu)\n",
    "    else:\n",
    "        device = torch.device('cpu')\n",
    "\n",
    "    model = model.to(device)\n",
    "\n",
    "    if end_index is None:\n",
    "        end_index = len(data)\n",
    "\n",
    "    i = 0\n",
    "    while i * batchsize < end_index-start_index:\n",
    "        start_index0 = start_index + i * batchsize \n",
    "        end_index0 = min(start_index + (i+1) * batchsize, end_index) \n",
    "        if param is None:\n",
    "            data1, logj1 = model.transform(data[start_index0:end_index0].to(device), label[start_index0:end_index0].to(device), start=start, end=end, param=param)\n",
    "        else:\n",
    "            data1, logj1 = model.transform(data[start_index0:end_index0].to(device), label[start_index0:end_index0].to(device), start=start, end=end, param=param[start_index0:end_index0].to(device))\n",
    "        data[start_index0:end_index0] = data1.to(data.device)\n",
    "        logj[start_index0:end_index0] = logj[start_index0:end_index0] + logj1.to(logj.device)\n",
    "        i += 1\n",
    "\n",
    "    del data1, logj1, model \n",
    "    if torch.cuda.is_available() and not nocuda:\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    return\n",
    "\n",
    "\n",
    "def conditional_transform_batch_model(model, data, label, batchsize, logj=None, start=0, end=None, param=None, pool=None, nocuda=False):\n",
    "\n",
    "    if logj is None:\n",
    "        logj = torch.zeros(len(data), device=data.device)\n",
    "\n",
    "    if pool is None: \n",
    "        _transform_batch_model(model, data, label, logj, 0, batchsize, start=start, end=end, param=param, nocuda=nocuda) \n",
    "    else:\n",
    "        if torch.cuda.is_available() and not nocuda:\n",
    "            nprocess = torch.cuda.device_count()\n",
    "        else:\n",
    "            nprocess = mp.cpu_count()\n",
    "        param0 = [(model, data, label, logj, i, batchsize, len(data)*i//nprocess, len(data)*(i+1)//nprocess, start, end, param, nocuda) for i in range(nprocess)]\n",
    "        pool.starmap(_conditional_transform_batch_model, param0)\n",
    "\n",
    "    return data, logj\n",
    "\n",
    "\n",
    "def _conditional_transform_batch_layer(layer, data, label, logj, index, batchsize, start_index=0, end_index=None, direction='forward', param=None, nocuda=False):\n",
    "\n",
    "    if torch.cuda.is_available() and not nocuda:\n",
    "        gpu = index % torch.cuda.device_count()\n",
    "        device = torch.device('cuda:%d'%gpu)\n",
    "    else:\n",
    "        device = torch.device('cpu')\n",
    "    \n",
    "    layer = layer.to(device)\n",
    "\n",
    "    if end_index is None:\n",
    "        end_index = len(data)\n",
    "\n",
    "    i = 0\n",
    "    while i * batchsize < end_index-start_index:\n",
    "        start_index0 = start_index + i * batchsize \n",
    "        end_index0 = min(start_index + (i+1) * batchsize, end_index) \n",
    "        if direction == 'forward': \n",
    "            if param is None:\n",
    "                data1, logj1 = layer.forward(data[start_index0:end_index0].to(device),label[start_index0:end_index0].to(device), param=param)\n",
    "            else:\n",
    "                data1, logj1 = layer.forward(data[start_index0:end_index0].to(device), label[start_index0:end_index0].to(device), param=param[start_index0:end_index0].to(device))\n",
    "        else: \n",
    "            if param is None:\n",
    "                data1, logj1 = layer.inverse(data[start_index0:end_index0].to(device),label[start_index0:end_index0].to(device), param=param)\n",
    "            else:\n",
    "                data1, logj1 = layer.inverse(data[start_index0:end_index0].to(device),label[start_index0:end_index0].to(device), param=param[start_index0:end_index0].to(device))\n",
    "        data[start_index0:end_index0] = data1.to(data.device)\n",
    "        logj[start_index0:end_index0] = logj[start_index0:end_index0] + logj1.to(logj.device)\n",
    "        i += 1\n",
    "\n",
    "    del data1, logj1, layer \n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    return\n",
    "\n",
    "\n",
    "def conditional_transform_batch_layer(layer, data, label, batchsize, logj=None, direction='forward', param=None, pool=None, nocuda=False):\n",
    "    assert direction in ['forward', 'inverse']\n",
    "    \n",
    "    if logj is None:\n",
    "        logj = torch.zeros(len(data), device=data.device)\n",
    "    \n",
    "    if pool is None: \n",
    "        _conditional_transform_batch_layer(layer, data, label, logj, 0, batchsize, direction=direction, param=param, nocuda=nocuda) \n",
    "    else:\n",
    "        if torch.cuda.is_available() and not nocuda:\n",
    "            nprocess = torch.cuda.device_count()\n",
    "        else:\n",
    "            nprocess = mp.cpu_count()\n",
    "        param0 = [(layer, data, label, logj, i, batchsize, len(data)*i//nprocess, len(data)*(i+1)//nprocess, direction, param, nocuda) for i in range(nprocess)]\n",
    "        pool.starmap(_conditional_transform_batch_layer, param0)\n",
    "    \n",
    "    return data, logj\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e826da77-4aa0-4076-9842-b05c14c033d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConditionalSINF(nn.Module):\n",
    "\n",
    "    #Sliced Iterative Normalizing Flow model\n",
    "    \n",
    "    def __init__(self, ndim, n_class):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.layer = nn.ModuleList([])\n",
    "        self.ndim = ndim\n",
    "        self.n_class = n_class\n",
    "    \n",
    "    def forward(self, data, label, start=0, end=None):\n",
    "        \n",
    "        if data.ndim == 1:\n",
    "            data = data.view(1,-1)\n",
    "        if end is None:\n",
    "            end = len(self.layer)\n",
    "        elif end < 0:\n",
    "            end += len(self.layer)\n",
    "        if start < 0:\n",
    "            start += len(self.layer)\n",
    "        \n",
    "        assert start >= 0 and end >= 0 and end >= start\n",
    "\n",
    "        logj = torch.zeros(data.shape[0], device=data.device)\n",
    "        \n",
    "        for i in range(start, end):\n",
    "            data, log_j = self.layer[i](data, param=label)\n",
    "            logj += log_j\n",
    "\n",
    "        return data, logj\n",
    "    \n",
    "    \n",
    "    def inverse(self, data, label, start=None, end=0, d_dz=None):\n",
    "\n",
    "        if data.ndim == 1:\n",
    "            data = data.view(1,-1)\n",
    "        if end < 0:\n",
    "            end += len(self.layer)\n",
    "        if start is None:\n",
    "            start = len(self.layer)\n",
    "        elif start < 0:\n",
    "            start += len(self.layer)\n",
    "        \n",
    "        assert start >= 0 and end >= 0 and end <= start\n",
    "\n",
    "        logj = torch.zeros(data.shape[0], device=data.device)\n",
    "        \n",
    "        for i in reversed(range(end, start)):\n",
    "            if d_dz is None:\n",
    "                data, log_j = self.layer[i].inverse(data, param=label)\n",
    "            else:\n",
    "                data, log_j, d_dz = self.layer[i].inverse(data,d_dz=d_dz, param=label)\n",
    "            logj += log_j\n",
    "\n",
    "        if d_dz is None:\n",
    "            return data, logj\n",
    "        else:\n",
    "            return data, logj, d_dz\n",
    "\n",
    "\n",
    "    def transform(self, data, label, start, end,):\n",
    "\n",
    "        if start is None:\n",
    "            return self.inverse(data=data, start=start, end=end, param=label) \n",
    "        elif end is None:\n",
    "            return self.forward(data=data, start=start, end=end, param=label) \n",
    "        elif start < 0:\n",
    "            start += len(self.layer)\n",
    "        elif end < 0:\n",
    "            end += len(self.layer)\n",
    "        \n",
    "        if start < 0:\n",
    "            start = 0\n",
    "        elif start > len(self.layer):\n",
    "            start = len(self.layer)\n",
    "        if end < 0:\n",
    "            end = 0\n",
    "        elif end > len(self.layer):\n",
    "            end = len(self.layer)\n",
    "\n",
    "        if start <= end:\n",
    "            return self.forward(data=data, start=start, end=end, param=label) \n",
    "        else:\n",
    "            return self.inverse(data=data, start=start, end=end, param=label) \n",
    "    \n",
    "    \n",
    "    def add_layer(self, layer, position=None):\n",
    "        \n",
    "        if position is None or position == len(self.layer):\n",
    "            self.layer.append(layer)\n",
    "        else:\n",
    "            if position < 0:\n",
    "                position += len(self.layer)\n",
    "            assert position >= 0 and position < len(self.layer)\n",
    "            self.layer.insert(position, layer)\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    \n",
    "    def delete_layer(self, position=-1):\n",
    "        \n",
    "        if position == -1 or position == len(self.layer)-1:\n",
    "            self.layer = self.layer[:-1]\n",
    "        else:\n",
    "            if position < 0:\n",
    "                position += len(self.layer)\n",
    "            assert position >= 0 and position < len(self.layer)-1\n",
    "            \n",
    "            for i in range(position, len(self.layer)-1):\n",
    "                self.layer._modules[str(i)] = self.layer._modules[str(i + 1)]\n",
    "            self.layer = self.layer[:-1]\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    \n",
    "    def evaluate_density(self, data, label, start=0, end=None):\n",
    "        \n",
    "        data, logj = self.forward(data, label, start=start, end=end)\n",
    "        logq = -self.ndim/2*torch.log(torch.tensor(2*math.pi)) - torch.sum(data.reshape(len(data), self.ndim)**2,  dim=1)/2\n",
    "        logp = logj + logq\n",
    "        \n",
    "        return logp\n",
    "\n",
    "\n",
    "    def loss(self, data, start=0, end=None, param=None):\n",
    "        return -torch.mean(self.evaluate_density(data, label, start=start, end=end, param=param))\n",
    "    \n",
    "    \n",
    "    def sample(self, nsample, label, start=None, end=0, device=torch.device('cuda'), param=None):\n",
    "\n",
    "        #device must be the same as the device of the model\n",
    "        \n",
    "        x       = torch.randn(nsample, self.ndim, device=device)\n",
    "        logq    = -self.ndim/2.*torch.log(torch.tensor(2.*math.pi)) - torch.sum(x**2,  dim=1)/2\n",
    "        x, logj = self.inverse(x, label, start=start, end=end, param=param)\n",
    "        logp    = logj + logq\n",
    "\n",
    "        return x, logp\n",
    "\n",
    "\n",
    "    def score(self, data, label, start=0, end=None, param=None):\n",
    "\n",
    "        #returns score = dlogp / dx\n",
    "\n",
    "        data.requires_grad_(True)\n",
    "        logp  = torch.sum(self.evaluate_density(data, label, start, end, param))\n",
    "        score = torch.autograd.grad(logp, data)[0]\n",
    "        data.requires_grad_(False)\n",
    "\n",
    "        return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b1398aea-6bd4-4da0-ab8b-0b5672a092b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ConditionalGIS(data_train, label_train, data_valid, label_valid, iteration=None, K=None, M=None, KDE=True, b_factor=1, alpha=None, bounds=None,max_iter=400,\n",
    "        edge_bins=None, ndata_A=None, MSWD_max_iter=None, NBfirstlayer=False, Whiten=True, batchsize=None, nocuda=False, patch=False, shape=None, model=None, verbose=True):\n",
    "    \n",
    "    '''\n",
    "    data_train: (ndata_train, ndim).\n",
    "    data_valid: (ndata_valid, ndim), optional. If provided, its logp will be used to determine the number of iterations.\n",
    "    iteration: integer, optional. The maximum number of GIS iterations. Required if data_valid is not provided.\n",
    "    K: integer, optional. The number of slices for each iteration. See max K-SWD in the SINF paper. 1 <= K <= ndim.\n",
    "    M: integer, optional. The number of spline knots for rational quadratic splines.\n",
    "    KDE: bool. Whether to use KDE for estimating 1D PDF. Recommended True.\n",
    "    b_factor: positive float number, optional. The multiplicative factor for KDE kernel width.\n",
    "    alpha: two non-negative float number in the format of (alpha1, alpha2), optional. Regularization parameter. See Equation 13 of SINF paper. alpha1 for interpolation, alpha2 for extrapolation slope. 0 <= alpha1,2 < 1. If not given, very heavy regularization will be used, which could result in slow training and a large number of iterations.\n",
    "    bounds: sequence, optional. In the format of [[x1_min, x1_max], [x2_min, x2_max], ..., [xd_min, xd_max]]. Represent infinity and negative infinity with None.\n",
    "    edge_bins: non-negative integer, optional. The number of spline knots at the boundary.\n",
    "    ndata_A: positive integer, optional. The number of training data used for fitting A (slice axes).\n",
    "    MSWD_max_iter: positive integer, optional. The maximum number of iterations for optimizing A (slice axes). See Algorithm 1 of SINF paper. Called L_iter in the paper.\n",
    "    NBfirstlayer: bool, optional. Whether to use Naive Bayes (no rotation) at the first layer.\n",
    "    Whiten: bool, optional. Whether to whiten the data before applying GIS.\n",
    "    batchsize: positive integer, optional. The batch size for transforming the data. Does not change the performance. Only saves the memory. \n",
    "    Useful when the data is too large and can't fit in the memory.\n",
    "    nocuda: bool, optional. Whether to use gpu.\n",
    "    patch: bool, optional. Whether to use patch-based modeling. Only useful for image datasets.\n",
    "    shape: sequence, optional. The shape of the image datasets, if patch is enabled.\n",
    "    model: GIS model, optional. Trained GIS model. If provided, new iterations will be added in the model.\n",
    "    verbose: bool, optional. Whether to print training information.\n",
    "    '''\n",
    "\n",
    "    assert data_valid is not None or iteration is not None\n",
    " \n",
    "    #hyperparameters\n",
    "    ndim    = data_train.shape[1]\n",
    "    nclass  = len(np.unique(label_train.cpu().numpy()))\n",
    "    print(ndim,nclass)\n",
    "\n",
    "    ndata = len(data_train)\n",
    "\n",
    "    if M is None:\n",
    "        M = max(min(200, int(ndata**0.5)), 50)\n",
    "    if alpha is None:\n",
    "        alpha = (1-0.02*math.log10(ndata), 1-0.001*math.log10(ndata))#).to(device)\n",
    "    if bounds is not None:\n",
    "        assert len(bounds) == ndim\n",
    "        for i in range(ndim):\n",
    "            assert len(bounds[i]) == 2\n",
    "    if edge_bins is None:\n",
    "        edge_bins = max(int(math.log10(ndata))-1, 0)\n",
    "    if batchsize is None:\n",
    "        batchsize = len(data_train)\n",
    "    if not patch:\n",
    "        if K is None:\n",
    "            if ndim <= 8 or ndata / float(ndim) < 20:\n",
    "                K = ndim\n",
    "            else:\n",
    "                K = 8\n",
    "        if ndata_A is None:\n",
    "            ndata_A = min(len(data_train), int(math.log10(ndim)*1e5))\n",
    "        if MSWD_max_iter is None:\n",
    "            MSWD_max_iter = min(round(ndata) // ndim, 200)\n",
    "    else:\n",
    "        assert shape[0] > 4 and shape[1] > 4\n",
    "        K0 = K\n",
    "        ndata_A0 = ndata_A\n",
    "        MSWD_max_iter0 = MSWD_max_iter\n",
    "\n",
    "        \n",
    "    best_accuracy = 0\n",
    "    #device\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() and not nocuda else \"cpu\")\n",
    "\n",
    "    \n",
    "    ### taking out logit transform and replacing log jacobian with zeros \n",
    "    logj_train    = torch.zeros(data_train.shape[0],device=device)\n",
    "    logj_valid    = torch.zeros(data_valid.shape[0],device=device)\n",
    "\n",
    "    #define the model\n",
    "    if model is None:\n",
    "        model = ConditionalSINF(ndim=ndim,n_class=nclass).requires_grad_(False).to(device)\n",
    "        if data_valid is not None:\n",
    "            best_logp_valid = -1e10\n",
    "            best_Nlayer     = 0\n",
    "            wait            = 0\n",
    "            maxwait         = 5 \n",
    "#     else:\n",
    "#         t = time.time()\n",
    "#         data_train, logj_train = conditional_transform_batch_model(model, data_train, batchsize, logj=logj_train, start=0, end=None, nocuda=nocuda)\n",
    "#         logp_train             = (torch.mean(logj_train) - ndim/2*torch.log(torch.tensor(2*math.pi)) - torch.mean(torch.sum(data_train**2,  dim=1)/2)).item()\n",
    "        \n",
    "#         if data_valid is not None:\n",
    "#             data_valid, logj_valid = conditional_transform_batch_model(model, data_valid, batchsize, logj=logj_valid, start=0, end=None, nocuda=nocuda)\n",
    "#             logp_valid             = (torch.mean(logj_valid) - ndim/2*torch.log(torch.tensor(2*math.pi)) - torch.mean(torch.sum(data_valid**2,  dim=1)/2)).item()\n",
    "#             print ('Initial logp:', logp_train, logp_valid, 'time:', time.time()-t, 'iteration:', len(model.layer))\n",
    "#         else:\n",
    "#             print ('Initial logp:', logp_train, 'time:', time.time()-t, 'iteration:', len(model.layer))\n",
    "\n",
    "    \n",
    "    #whiten\n",
    "    if Whiten:\n",
    "        layer = whiten(ndim_data=ndim, scale=True, ndim_latent=ndim).requires_grad_(False).to(device)\n",
    "        layer.fit(data_train)\n",
    "\n",
    "        data_train, logj_train0 = layer(data_train)\n",
    "        logj_train += logj_train0\n",
    "\n",
    "        logp_train = (torch.mean(logj_train) - ndim/2*torch.log(torch.tensor(2*math.pi)) - torch.mean(torch.sum(data_train**2,  dim=1)/2)).item()\n",
    "        \n",
    "        if data_valid is not None:\n",
    "            data_valid, logj_valid0 = layer(data_valid)\n",
    "            logj_valid += logj_valid0\n",
    "            logp_valid = (torch.mean(logj_valid) - ndim/2*torch.log(torch.tensor(2*math.pi)) - torch.mean(torch.sum(data_valid**2,  dim=1)/2)).item()\n",
    "            if logp_valid > best_logp_valid:\n",
    "                best_logp_valid = logp_valid\n",
    "                best_Nlayer = len(model.layer)\n",
    "\n",
    "        model.add_layer(layer)\n",
    "        if verbose:\n",
    "            if data_valid is not None:\n",
    "                print('After whiten logp:', logp_train, logp_valid)\n",
    "            else:\n",
    "                print('After whiten logp:', logp_train)\n",
    "\n",
    "                \n",
    "                \n",
    "    data_train    = data_train[None,:].repeat_interleave(nclass, axis=0)\n",
    "    data_valid    = data_valid[None,:].repeat_interleave(nclass, axis=0)\n",
    "\n",
    "    logj_train    = logj_train[None,:].repeat_interleave(nclass, axis=0)\n",
    "    logj_valid    = logj_valid[None,:].repeat_interleave(nclass, axis=0)\n",
    "\n",
    "    #GIS iterations\n",
    "    \n",
    "    ii=0\n",
    "    while True:\n",
    "        t = time.time()\n",
    "\n",
    "        if NBfirstlayer:\n",
    "            layer = ConditionalSlicedTransport_discrete(ndim=ndim, n_class = nclass, K=ndim, M=M).requires_grad_(False).to(device)\n",
    "        else:\n",
    "            layer = ConditionalSlicedTransport_discrete(ndim=ndim, n_class = nclass, K=K, M=M).requires_grad_(False).to(device)\n",
    "\n",
    "        #fit the layer\n",
    "        if NBfirstlayer:\n",
    "            layer.A[:] = torch.eye(ndim).to(device)\n",
    "            NBfirstlayer = False\n",
    "        elif ndim > 1:\n",
    "            layer.fit_A(data_train[label_train, torch.arange(data_train.shape[1]).to(device)], MSWD_max_iter=MSWD_max_iter, verbose=verbose)\n",
    "\n",
    "        layer.fit_spline(data_train[label_train, torch.arange(data_train.shape[1]).to(device)], label_train, edge_bins=edge_bins, derivclip=1, alpha=alpha, KDE=KDE, b_factor=b_factor, verbose=False)\n",
    "        \n",
    "        for label in range(nclass):\n",
    "            data_train[label], logj_train1 = layer(data_train[label], torch.ones(data_train.shape[1], dtype=torch.int, device=data_train.device)*label)\n",
    "            logj_train[label] = logj_train[label] + logj_train1\n",
    "\n",
    "            data_valid[label], logj_valid1 = layer(data_valid[label], torch.ones(data_valid.shape[1], dtype=torch.int, device=data_valid.device)*label)\n",
    "            logj_valid[label] = logj_valid[label] + logj_valid1\n",
    "\n",
    "        model.add_layer(layer)\n",
    "\n",
    "\n",
    "#     for label in range(n_class):\n",
    "#         data_train[label], logj_train1 = layer(data_train[label], torch.ones(data_train.shape[1], dtype=torch.int, device=data_train.device)*label)\n",
    "#         logp_train[label]              = logj_train - torch.sum(data_train**2,  dim=1)/2- ndim/2*torch.log(torch.tensor(2*math.pi))\n",
    "\n",
    "#         data_valid[label], logj_valid1 = layer(data_validate[label], torch.ones(data_validate.shape[1], dtype=torch.int, device=data_validate.device)*label)\n",
    "#         logp_valid[label]              = logj_valid\n",
    "\n",
    "#         data_test[label], logj_test1 = layer(data_test[label], torch.ones(data_test.shape[1], dtype=torch.int, device=data_test.device)*label)\n",
    "#         logj_test[label]             = logj_test[label] + logj_test1\n",
    "        \n",
    "        logp_train = (torch.mean(logj_train[label_train, torch.arange(data_train.shape[1]).to(device)]) - ndim/2*torch.log(torch.tensor(2*math.pi)) - torch.mean(torch.sum(data_train[label_train, torch.arange(data_train.shape[1]).to(device)]**2,  dim=1)/2)).item()\n",
    "        logp_valid = (torch.mean(logj_valid[label_valid, torch.arange(data_valid.shape[1]).to(device)]) - ndim/2*torch.log(torch.tensor(2*math.pi)) - torch.mean(torch.sum(data_valid[label_valid, torch.arange(data_valid.shape[1]).to(device)]**2,  dim=1)/2)).item()\n",
    "\n",
    "        \n",
    "#         logp_valid_label = torch.ones([nclass,label_valid.shape[0]], device=data_valid.device)\n",
    "#         if data_valid is not None:\n",
    "#             for mm in range(nclass):\n",
    "#                 data_valid, logj_valid = conditional_transform_batch_layer(layer, data_valid, torch.ones(label_valid.shape, dtype=torch.int, device=data_valid.device)*mm, batchsize, logj=logj_valid, direction='forward', nocuda=nocuda)\n",
    "#                 logp_valid = (logj_valid - torch.sum(data_valid**2,  dim=1)/2)- ndim/2*torch.log(torch.tensor(2*math.pi))\n",
    "#                 logp_valid_label[mm] = logp_valid\n",
    "#             predict_label_valid = torch.argmax(logp_valid_label,dim=0)\n",
    "#             accuracy = torch.sum(predict_label_valid==label_valid).item() / len(label_valid)\n",
    "#             print('valid accuracy: ', torch.sum(predict_label_valid==label_valid).item() / len(label_valid))\n",
    "    \n",
    "#             data_valid, logj_valid = conditional_transform_batch_layer(layer, data_valid, label_valid, batchsize, logj=logj_valid, direction='forward', nocuda=nocuda)\n",
    "#             logp_valid = (torch.mean(logj_valid) - ndim/2*torch.log(torch.tensor(2*math.pi)) - torch.mean(torch.sum(data_valid**2,  dim=1)/2)).item()\n",
    "\n",
    "            \n",
    "        if logp_valid > best_logp_valid:\n",
    "            best_logp_valid = logp_valid\n",
    "            best_Nlayer = len(model.layer)\n",
    "            wait = 0\n",
    "        else:\n",
    "            wait += 1\n",
    "        if wait == maxwait:\n",
    "            model.layer = model.layer[:best_Nlayer]\n",
    "            break\n",
    "        if ii>max_iter:\n",
    "            break\n",
    "\n",
    "#         if  accuracy > best_accuracy:\n",
    "#             best_accuracy = accuracy\n",
    "#             best_Nlayer_acc = len(model.layer)\n",
    "#             wait_acc = 0\n",
    "#         else:\n",
    "#             wait_acc += 1\n",
    "#         if wait_acc == maxwait:\n",
    "#             model.layer = model.layer[:best_Nlayer_acc]\n",
    "#             break\n",
    "\n",
    "        if verbose:\n",
    "            if data_valid is not None: \n",
    "                print ('logp:', logp_train, logp_valid, 'time:', time.time()-t, 'iteration:', len(model.layer), 'best:', best_Nlayer)\n",
    "            else:\n",
    "                print ('logp:', logp_train, 'time:', time.time()-t, 'iteration:', len(model.layer))\n",
    "\n",
    "        if iteration is not None and len(model.layer) >= iteration:\n",
    "            if data_valid is not None:\n",
    "                model.layer = model.layer[:best_Nlayer]\n",
    "            break\n",
    "        ii+=1\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "87a5a074-cad9-4b30-a6a8-9856bf34f973",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 2\n",
      "After whiten logp: -0.38617706298828125 -0.45709872245788574\n",
      "Fit A: Time: 0.04889559745788574 Wasserstein Distance: [0.2552861273288727, 0.20752927660942078]\n",
      "logp: -0.3756265640258789 -0.4461078643798828 time: 0.08711671829223633 iteration: 2 best: 2\n",
      "Fit A: Time: 0.0034019947052001953 Wasserstein Distance: [0.16684222221374512, 0.13402687013149261]\n",
      "logp: -0.3695746660232544 -0.447503924369812 time: 0.01874089241027832 iteration: 3 best: 2\n",
      "Fit A: Time: 0.01674509048461914 Wasserstein Distance: [0.25232863426208496, 0.2037918120622635]\n",
      "logp: -0.35963505506515503 -0.437359094619751 time: 0.03340435028076172 iteration: 4 best: 4\n",
      "Fit A: Time: 0.016663074493408203 Wasserstein Distance: [0.24993538856506348, 0.19993917644023895]\n",
      "logp: -0.3500182032585144 -0.42757856845855713 time: 0.03252434730529785 iteration: 5 best: 5\n",
      "Fit A: Time: 0.0200197696685791 Wasserstein Distance: [0.2461789846420288, 0.19777953624725342]\n",
      "logp: -0.3407015800476074 -0.418102502822876 time: 0.03626847267150879 iteration: 6 best: 6\n",
      "Fit A: Time: 0.012958765029907227 Wasserstein Distance: [0.2491689920425415, 0.18630832433700562]\n",
      "logp: -0.33177709579467773 -0.4093754291534424 time: 0.02820134162902832 iteration: 7 best: 7\n",
      "Fit A: Time: 0.02016592025756836 Wasserstein Distance: [0.24005715548992157, 0.1921902894973755]\n",
      "logp: -0.3230302333831787 -0.4000462293624878 time: 0.03555583953857422 iteration: 8 best: 8\n",
      "Fit A: Time: 0.018670320510864258 Wasserstein Distance: [0.2371969074010849, 0.1893044263124466]\n",
      "logp: -0.3147403597831726 -0.39148175716400146 time: 0.03448772430419922 iteration: 9 best: 9\n",
      "Fit A: Time: 0.028029918670654297 Wasserstein Distance: [0.2346792370080948, 0.18611519038677216]\n",
      "logp: -0.30654484033584595 -0.38321566581726074 time: 0.044931888580322266 iteration: 10 best: 10\n",
      "Fit A: Time: 0.017552852630615234 Wasserstein Distance: [0.23656070232391357, 0.177120178937912]\n",
      "logp: -0.2986631393432617 -0.37552452087402344 time: 0.034186363220214844 iteration: 11 best: 11\n",
      "Fit A: Time: 0.016607284545898438 Wasserstein Distance: [0.22999027371406555, 0.1796533763408661]\n",
      "logp: -0.2909783124923706 -0.3678349256515503 time: 0.03264331817626953 iteration: 12 best: 12\n",
      "Fit A: Time: 0.018540620803833008 Wasserstein Distance: [0.22794710099697113, 0.1761973649263382]\n",
      "logp: -0.2837525010108948 -0.3607074022293091 time: 0.03514909744262695 iteration: 13 best: 13\n",
      "Fit A: Time: 0.02159261703491211 Wasserstein Distance: [0.23019038140773773, 0.16710399091243744]\n",
      "logp: -0.2766155004501343 -0.35386955738067627 time: 0.0413815975189209 iteration: 14 best: 14\n",
      "Fit A: Time: 0.025167226791381836 Wasserstein Distance: [0.22256329655647278, 0.171211376786232]\n",
      "logp: -0.26942557096481323 -0.3469674587249756 time: 0.042200326919555664 iteration: 15 best: 15\n",
      "Fit A: Time: 0.009407758712768555 Wasserstein Distance: [0.1723136454820633, 0.12309226393699646]\n",
      "logp: -0.26418453454971313 -0.34780824184417725 time: 0.02772068977355957 iteration: 16 best: 15\n",
      "Fit A: Time: 0.01685953140258789 Wasserstein Distance: [0.22522275149822235, 0.16165833175182343]\n",
      "logp: -0.2575361132621765 -0.3414602279663086 time: 0.032541513442993164 iteration: 17 best: 17\n",
      "Fit A: Time: 0.008112907409667969 Wasserstein Distance: [0.22300118207931519, 0.15886349976062775]\n",
      "logp: -0.2510988712310791 -0.33535170555114746 time: 0.02495551109313965 iteration: 18 best: 18\n",
      "Fit A: Time: 0.009789705276489258 Wasserstein Distance: [0.22049900889396667, 0.15647317469120026]\n",
      "logp: -0.24486172199249268 -0.32945752143859863 time: 0.02425837516784668 iteration: 19 best: 19\n",
      "Fit A: Time: 0.015364885330200195 Wasserstein Distance: [0.17234563827514648, 0.11881235986948013]\n",
      "logp: -0.23997151851654053 -0.33001554012298584 time: 0.029013633728027344 iteration: 20 best: 19\n",
      "Fit A: Time: 0.02069544792175293 Wasserstein Distance: [0.21759183704853058, 0.15422365069389343]\n",
      "logp: -0.23396259546279907 -0.32431936264038086 time: 0.03820347785949707 iteration: 21 best: 21\n",
      "Fit A: Time: 0.011941909790039062 Wasserstein Distance: [0.21508949995040894, 0.15196025371551514]\n",
      "logp: -0.22814011573791504 -0.3188209533691406 time: 0.026328563690185547 iteration: 22 best: 22\n",
      "Fit A: Time: 0.012243270874023438 Wasserstein Distance: [0.1670200526714325, 0.12046705931425095]\n",
      "logp: -0.22320926189422607 -0.32054364681243896 time: 0.027750492095947266 iteration: 23 best: 22\n",
      "Fit A: Time: 0.012072086334228516 Wasserstein Distance: [0.20730867981910706, 0.1563243418931961]\n",
      "logp: -0.21753555536270142 -0.3153810501098633 time: 0.02666640281677246 iteration: 24 best: 24\n",
      "Fit A: Time: 0.009922266006469727 Wasserstein Distance: [0.16745944321155548, 0.11637403070926666]\n",
      "logp: -0.212926983833313 -0.31640803813934326 time: 0.025942087173461914 iteration: 25 best: 24\n",
      "Fit A: Time: 0.017073392868041992 Wasserstein Distance: [0.20733006298542023, 0.15076684951782227]\n",
      "logp: -0.20748591423034668 -0.3111112117767334 time: 0.03247475624084473 iteration: 26 best: 26\n",
      "Fit A: Time: 0.007242918014526367 Wasserstein Distance: [0.16588307917118073, 0.11398967355489731]\n",
      "logp: -0.2030983567237854 -0.3120180368423462 time: 0.021978139877319336 iteration: 27 best: 26\n",
      "Fit A: Time: 0.013095855712890625 Wasserstein Distance: [0.20673592388629913, 0.14545276761054993]\n",
      "logp: -0.19787997007369995 -0.30707406997680664 time: 0.027528047561645508 iteration: 28 best: 28\n",
      "Fit A: Time: 0.01631784439086914 Wasserstein Distance: [0.20247849822044373, 0.146040141582489]\n",
      "logp: -0.1927970051765442 -0.3021101951599121 time: 0.03078603744506836 iteration: 29 best: 29\n",
      "Fit A: Time: 0.01692485809326172 Wasserstein Distance: [0.20003916323184967, 0.14393915235996246]\n",
      "logp: -0.18787157535552979 -0.29731321334838867 time: 0.031420230865478516 iteration: 30 best: 30\n",
      "Fit A: Time: 0.010319709777832031 Wasserstein Distance: [0.2009771168231964, 0.13656187057495117]\n",
      "logp: -0.18314450979232788 -0.29297852516174316 time: 0.025168895721435547 iteration: 31 best: 31\n",
      "Fit A: Time: 0.008905649185180664 Wasserstein Distance: [0.16410046815872192, 0.11181723326444626]\n",
      "logp: -0.17889970541000366 -0.2940319776535034 time: 0.02347731590270996 iteration: 32 best: 31\n",
      "Fit A: Time: 0.015881061553955078 Wasserstein Distance: [0.1960873156785965, 0.13818353414535522]\n",
      "logp: -0.1743069887161255 -0.2897064685821533 time: 0.030229806900024414 iteration: 33 best: 33\n",
      "Fit A: Time: 0.012080192565917969 Wasserstein Distance: [0.16189508140087128, 0.10995247960090637]\n",
      "logp: -0.17021310329437256 -0.2908010482788086 time: 0.026842594146728516 iteration: 34 best: 33\n",
      "Fit A: Time: 0.013251781463623047 Wasserstein Distance: [0.18964241445064545, 0.14143356680870056]\n",
      "logp: -0.1657242774963379 -0.28681838512420654 time: 0.02712225914001465 iteration: 35 best: 35\n",
      "Fit A: Time: 0.012258291244506836 Wasserstein Distance: [0.1658470183610916, 0.09589599817991257]\n",
      "logp: -0.1621219515800476 -0.2888987064361572 time: 0.025758028030395508 iteration: 36 best: 35\n",
      "Fit A: Time: 0.011159420013427734 Wasserstein Distance: [0.15533840656280518, 0.10958199948072433]\n",
      "logp: -0.15798813104629517 -0.29132509231567383 time: 0.02624964714050293 iteration: 37 best: 35\n",
      "Fit A: Time: 0.0007791519165039062 Wasserstein Distance: [0.1882706880569458, 0.13661529123783112]\n",
      "logp: -0.15374010801315308 -0.2872711420059204 time: 0.015955209732055664 iteration: 38 best: 35\n",
      "Fit A: Time: 0.017459630966186523 Wasserstein Distance: [0.15609556436538696, 0.10625801980495453]\n",
      "logp: -0.14990782737731934 -0.28866851329803467 time: 0.03298830986022949 iteration: 39 best: 35\n",
      "Fit A: Time: 0.013082265853881836 Wasserstein Distance: [0.15387137234210968, 0.10477513819932938]\n"
     ]
    }
   ],
   "source": [
    "model = train_ConditionalGIS(train_x, train_y,valid_x, valid_y, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5c1cc5e4-4d1a-4524-a2ce-3a2acd85d863",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "No CUDA GPUs are available",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-6fa505f33264>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-10-e92252681a0b>\u001b[0m in \u001b[0;36msample\u001b[0;34m(self, nsample, label, start, end, device, param)\u001b[0m\n\u001b[1;32m    133\u001b[0m         \u001b[0;31m#device must be the same as the device of the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 135\u001b[0;31m         \u001b[0mx\u001b[0m       \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnsample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    136\u001b[0m         \u001b[0mlogq\u001b[0m    \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m2.\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2.\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mmath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m         \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlogj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minverse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.conda/envs/pytorch/lib/python3.7/site-packages/torch/cuda/__init__.py\u001b[0m in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    168\u001b[0m         \u001b[0;31m# This function throws if there's a driver initialization error, no GPUs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    169\u001b[0m         \u001b[0;31m# are found or any other error occurs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 170\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cuda_init\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    171\u001b[0m         \u001b[0;31m# Some of the queued calls may reentrantly call _lazy_init();\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    172\u001b[0m         \u001b[0;31m# we need to just return without initializing in that case.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: No CUDA GPUs are available"
     ]
    }
   ],
   "source": [
    "model.sample(10, label=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eddc8126-ab76-4da3-aceb-ea7992e8d3c0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
