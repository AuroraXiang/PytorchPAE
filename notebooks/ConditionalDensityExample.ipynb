{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e47cd393-a7ab-450d-952f-43c889a538fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import numpy as np\n",
    "sys.path.append('/global/u2/v/vboehm/codes/SIG_GIS/')\n",
    "from sig_gis import *\n",
    "from sig_gis.GIS import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "39d2a8cd-39a9-4fb7-ae0d-0d3a45c06734",
   "metadata": {},
   "outputs": [],
   "source": [
    "#some fudge data\n",
    "train_x  = np.random.random((100,2))\n",
    "#some fudge labels\n",
    "train_y = np.append(np.ones(50),np.zeros(50))\n",
    "np.random.shuffle(train_y)\n",
    "\n",
    "#some fudge data\n",
    "valid_x  = np.random.random((50,2))\n",
    "#some fudge labels\n",
    "valid_y = np.append(np.ones(25),np.zeros(25))\n",
    "np.random.shuffle(valid_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "372bf240-0fa4-4787-8e25-0c44c114644f",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "96c6bfc3-5923-49db-bb15-b02afa5d2559",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = torch.Tensor(train_x).to(device) \n",
    "train_y = torch.Tensor(train_y).to(torch.long).to(device)\n",
    "\n",
    "valid_x = torch.Tensor(valid_x).to(device) \n",
    "valid_y = torch.Tensor(valid_y).to(torch.long).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7aa62784-22f7-437e-b10d-585a9440991c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _conditional_transform_batch_model(model, data, label, logj, index, batchsize, start_index=0, end_index=None, start=0, end=None, param=None, nocuda=False):\n",
    "\n",
    "    if torch.cuda.is_available() and not nocuda:\n",
    "        gpu    = index % torch.cuda.device_count()\n",
    "        device = torch.device('cuda:%d'%gpu)\n",
    "    else:\n",
    "        device = torch.device('cpu')\n",
    "\n",
    "    model = model.to(device)\n",
    "\n",
    "    if end_index is None:\n",
    "        end_index = len(data)\n",
    "\n",
    "    i = 0\n",
    "    while i * batchsize < end_index-start_index:\n",
    "        start_index0 = start_index + i * batchsize \n",
    "        end_index0 = min(start_index + (i+1) * batchsize, end_index) \n",
    "        if param is None:\n",
    "            data1, logj1 = model.transform(data[start_index0:end_index0].to(device), label[start_index0:end_index0].to(device), start=start, end=end, param=param)\n",
    "        else:\n",
    "            data1, logj1 = model.transform(data[start_index0:end_index0].to(device), label[start_index0:end_index0].to(device), start=start, end=end, param=param[start_index0:end_index0].to(device))\n",
    "        data[start_index0:end_index0] = data1.to(data.device)\n",
    "        logj[start_index0:end_index0] = logj[start_index0:end_index0] + logj1.to(logj.device)\n",
    "        i += 1\n",
    "\n",
    "    del data1, logj1, model \n",
    "    if torch.cuda.is_available() and not nocuda:\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    return\n",
    "\n",
    "\n",
    "def conditional_transform_batch_model(model, data, label, batchsize, logj=None, start=0, end=None, param=None, pool=None, nocuda=False):\n",
    "\n",
    "    if logj is None:\n",
    "        logj = torch.zeros(len(data), device=data.device)\n",
    "\n",
    "    if pool is None: \n",
    "        _transform_batch_model(model, data, label, logj, 0, batchsize, start=start, end=end, param=param, nocuda=nocuda) \n",
    "    else:\n",
    "        if torch.cuda.is_available() and not nocuda:\n",
    "            nprocess = torch.cuda.device_count()\n",
    "        else:\n",
    "            nprocess = mp.cpu_count()\n",
    "        param0 = [(model, data, label, logj, i, batchsize, len(data)*i//nprocess, len(data)*(i+1)//nprocess, start, end, param, nocuda) for i in range(nprocess)]\n",
    "        pool.starmap(_conditional_transform_batch_model, param0)\n",
    "\n",
    "    return data, logj\n",
    "\n",
    "\n",
    "def _conditional_transform_batch_layer(layer, data, label, logj, index, batchsize, start_index=0, end_index=None, direction='forward', param=None, nocuda=False):\n",
    "\n",
    "    if torch.cuda.is_available() and not nocuda:\n",
    "        gpu = index % torch.cuda.device_count()\n",
    "        device = torch.device('cuda:%d'%gpu)\n",
    "    else:\n",
    "        device = torch.device('cpu')\n",
    "    \n",
    "    layer = layer.to(device)\n",
    "\n",
    "    if end_index is None:\n",
    "        end_index = len(data)\n",
    "\n",
    "    i = 0\n",
    "    while i * batchsize < end_index-start_index:\n",
    "        start_index0 = start_index + i * batchsize \n",
    "        end_index0 = min(start_index + (i+1) * batchsize, end_index) \n",
    "        if direction == 'forward': \n",
    "            if param is None:\n",
    "                data1, logj1 = layer.forward(data[start_index0:end_index0].to(device),label[start_index0:end_index0].to(device), param=param)\n",
    "            else:\n",
    "                data1, logj1 = layer.forward(data[start_index0:end_index0].to(device), label[start_index0:end_index0].to(device), param=param[start_index0:end_index0].to(device))\n",
    "        else: \n",
    "            if param is None:\n",
    "                data1, logj1 = layer.inverse(data[start_index0:end_index0].to(device),label[start_index0:end_index0].to(device), param=param)\n",
    "            else:\n",
    "                data1, logj1 = layer.inverse(data[start_index0:end_index0].to(device),label[start_index0:end_index0].to(device), param=param[start_index0:end_index0].to(device))\n",
    "        data[start_index0:end_index0] = data1.to(data.device)\n",
    "        logj[start_index0:end_index0] = logj[start_index0:end_index0] + logj1.to(logj.device)\n",
    "        i += 1\n",
    "\n",
    "    del data1, logj1, layer \n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    return\n",
    "\n",
    "\n",
    "def conditional_transform_batch_layer(layer, data, label, batchsize, logj=None, direction='forward', param=None, pool=None, nocuda=False):\n",
    "    assert direction in ['forward', 'inverse']\n",
    "    \n",
    "    if logj is None:\n",
    "        logj = torch.zeros(len(data), device=data.device)\n",
    "    \n",
    "    if pool is None: \n",
    "        _conditional_transform_batch_layer(layer, data, label, logj, 0, batchsize, direction=direction, param=param, nocuda=nocuda) \n",
    "    else:\n",
    "        if torch.cuda.is_available() and not nocuda:\n",
    "            nprocess = torch.cuda.device_count()\n",
    "        else:\n",
    "            nprocess = mp.cpu_count()\n",
    "        param0 = [(layer, data, label, logj, i, batchsize, len(data)*i//nprocess, len(data)*(i+1)//nprocess, direction, param, nocuda) for i in range(nprocess)]\n",
    "        pool.starmap(_conditional_transform_batch_layer, param0)\n",
    "    \n",
    "    return data, logj\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e826da77-4aa0-4076-9842-b05c14c033d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConditionalSINF(nn.Module):\n",
    "\n",
    "    #Sliced Iterative Normalizing Flow model\n",
    "    \n",
    "    def __init__(self, ndim, n_class):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.layer = nn.ModuleList([])\n",
    "        self.ndim = ndim\n",
    "        self.n_class = n_class\n",
    "    \n",
    "    def forward(self, data, label, start=0, end=None):\n",
    "        \n",
    "        if data.ndim == 1:\n",
    "            data = data.view(1,-1)\n",
    "        if end is None:\n",
    "            end = len(self.layer)\n",
    "        elif end < 0:\n",
    "            end += len(self.layer)\n",
    "        if start < 0:\n",
    "            start += len(self.layer)\n",
    "        \n",
    "        assert start >= 0 and end >= 0 and end >= start\n",
    "\n",
    "        logj = torch.zeros(data.shape[0], device=data.device)\n",
    "        \n",
    "        for i in range(start, end):\n",
    "            data, log_j = self.layer[i](data, param=label)\n",
    "            logj += log_j\n",
    "\n",
    "        return data, logj\n",
    "    \n",
    "    \n",
    "    def inverse(self, data, label, start=None, end=0, d_dz=None):\n",
    "\n",
    "        if data.ndim == 1:\n",
    "            data = data.view(1,-1)\n",
    "        if end < 0:\n",
    "            end += len(self.layer)\n",
    "        if start is None:\n",
    "            start = len(self.layer)\n",
    "        elif start < 0:\n",
    "            start += len(self.layer)\n",
    "        \n",
    "        assert start >= 0 and end >= 0 and end <= start\n",
    "\n",
    "        logj = torch.zeros(data.shape[0], device=data.device)\n",
    "        \n",
    "        for i in reversed(range(end, start)):\n",
    "            if d_dz is None:\n",
    "                data, log_j = self.layer[i].inverse(data, param=label)\n",
    "            else:\n",
    "                data, log_j, d_dz = self.layer[i].inverse(data,d_dz=d_dz, param=label)\n",
    "            logj += log_j\n",
    "\n",
    "        if d_dz is None:\n",
    "            return data, logj\n",
    "        else:\n",
    "            return data, logj, d_dz\n",
    "\n",
    "\n",
    "    def transform(self, data, label, start, end,):\n",
    "\n",
    "        if start is None:\n",
    "            return self.inverse(data=data, start=start, end=end, param=label) \n",
    "        elif end is None:\n",
    "            return self.forward(data=data, start=start, end=end, param=label) \n",
    "        elif start < 0:\n",
    "            start += len(self.layer)\n",
    "        elif end < 0:\n",
    "            end += len(self.layer)\n",
    "        \n",
    "        if start < 0:\n",
    "            start = 0\n",
    "        elif start > len(self.layer):\n",
    "            start = len(self.layer)\n",
    "        if end < 0:\n",
    "            end = 0\n",
    "        elif end > len(self.layer):\n",
    "            end = len(self.layer)\n",
    "\n",
    "        if start <= end:\n",
    "            return self.forward(data=data, start=start, end=end, param=label) \n",
    "        else:\n",
    "            return self.inverse(data=data, start=start, end=end, param=label) \n",
    "    \n",
    "    \n",
    "    def add_layer(self, layer, position=None):\n",
    "        \n",
    "        if position is None or position == len(self.layer):\n",
    "            self.layer.append(layer)\n",
    "        else:\n",
    "            if position < 0:\n",
    "                position += len(self.layer)\n",
    "            assert position >= 0 and position < len(self.layer)\n",
    "            self.layer.insert(position, layer)\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    \n",
    "    def delete_layer(self, position=-1):\n",
    "        \n",
    "        if position == -1 or position == len(self.layer)-1:\n",
    "            self.layer = self.layer[:-1]\n",
    "        else:\n",
    "            if position < 0:\n",
    "                position += len(self.layer)\n",
    "            assert position >= 0 and position < len(self.layer)-1\n",
    "            \n",
    "            for i in range(position, len(self.layer)-1):\n",
    "                self.layer._modules[str(i)] = self.layer._modules[str(i + 1)]\n",
    "            self.layer = self.layer[:-1]\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    \n",
    "    def evaluate_density(self, data, label, start=0, end=None):\n",
    "        \n",
    "        data, logj = self.forward(data, label, start=start, end=end)\n",
    "        logq = -self.ndim/2*torch.log(torch.tensor(2*math.pi)) - torch.sum(data.reshape(len(data), self.ndim)**2,  dim=1)/2\n",
    "        logp = logj + logq\n",
    "        \n",
    "        return logp\n",
    "\n",
    "\n",
    "    def loss(self, data, start=0, end=None, param=None):\n",
    "        return -torch.mean(self.evaluate_density(data, label, start=start, end=end, param=param))\n",
    "    \n",
    "    \n",
    "    def sample(self, nsample, start=None, end=0, device=torch.device('cuda'), param=None):\n",
    "\n",
    "        #device must be the same as the device of the model\n",
    "        \n",
    "        x       = torch.randn(nsample, self.ndim, device=device)\n",
    "        logq    = -self.ndim/2.*torch.log(torch.tensor(2.*math.pi)) - torch.sum(x**2,  dim=1)/2\n",
    "        x, logj = self.inverse(x, start=start, end=end, param=param)\n",
    "        logp    = logj + logq\n",
    "\n",
    "        return x, logp\n",
    "\n",
    "\n",
    "    def score(self, data, label, start=0, end=None, param=None):\n",
    "\n",
    "        #returns score = dlogp / dx\n",
    "\n",
    "        data.requires_grad_(True)\n",
    "        logp  = torch.sum(self.evaluate_density(data, label, start, end, param))\n",
    "        score = torch.autograd.grad(logp, data)[0]\n",
    "        data.requires_grad_(False)\n",
    "\n",
    "        return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b1398aea-6bd4-4da0-ab8b-0b5672a092b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ConditionalGIS(data_train, label_train, data_valid, label_valid, iteration=None, K=None, M=None, KDE=True, b_factor=1, alpha=None, bounds=None,max_iter=400,\n",
    "        edge_bins=None, ndata_A=None, MSWD_max_iter=None, NBfirstlayer=False, Whiten=True, batchsize=None, nocuda=False, patch=False, shape=None, model=None, verbose=True):\n",
    "    \n",
    "    '''\n",
    "    data_train: (ndata_train, ndim).\n",
    "    data_valid: (ndata_valid, ndim), optional. If provided, its logp will be used to determine the number of iterations.\n",
    "    iteration: integer, optional. The maximum number of GIS iterations. Required if data_valid is not provided.\n",
    "    K: integer, optional. The number of slices for each iteration. See max K-SWD in the SINF paper. 1 <= K <= ndim.\n",
    "    M: integer, optional. The number of spline knots for rational quadratic splines.\n",
    "    KDE: bool. Whether to use KDE for estimating 1D PDF. Recommended True.\n",
    "    b_factor: positive float number, optional. The multiplicative factor for KDE kernel width.\n",
    "    alpha: two non-negative float number in the format of (alpha1, alpha2), optional. Regularization parameter. See Equation 13 of SINF paper. alpha1 for interpolation, alpha2 for extrapolation slope. 0 <= alpha1,2 < 1. If not given, very heavy regularization will be used, which could result in slow training and a large number of iterations.\n",
    "    bounds: sequence, optional. In the format of [[x1_min, x1_max], [x2_min, x2_max], ..., [xd_min, xd_max]]. Represent infinity and negative infinity with None.\n",
    "    edge_bins: non-negative integer, optional. The number of spline knots at the boundary.\n",
    "    ndata_A: positive integer, optional. The number of training data used for fitting A (slice axes).\n",
    "    MSWD_max_iter: positive integer, optional. The maximum number of iterations for optimizing A (slice axes). See Algorithm 1 of SINF paper. Called L_iter in the paper.\n",
    "    NBfirstlayer: bool, optional. Whether to use Naive Bayes (no rotation) at the first layer.\n",
    "    Whiten: bool, optional. Whether to whiten the data before applying GIS.\n",
    "    batchsize: positive integer, optional. The batch size for transforming the data. Does not change the performance. Only saves the memory. \n",
    "    Useful when the data is too large and can't fit in the memory.\n",
    "    nocuda: bool, optional. Whether to use gpu.\n",
    "    patch: bool, optional. Whether to use patch-based modeling. Only useful for image datasets.\n",
    "    shape: sequence, optional. The shape of the image datasets, if patch is enabled.\n",
    "    model: GIS model, optional. Trained GIS model. If provided, new iterations will be added in the model.\n",
    "    verbose: bool, optional. Whether to print training information.\n",
    "    '''\n",
    "\n",
    "    assert data_valid is not None or iteration is not None\n",
    " \n",
    "    #hyperparameters\n",
    "    ndim    = data_train.shape[1]\n",
    "    nclass  = len(np.unique(label_train.cpu().numpy()))\n",
    "    print(ndim,nclass)\n",
    "\n",
    "    ndata = len(data_train)\n",
    "\n",
    "    if M is None:\n",
    "        M = max(min(200, int(ndata**0.5)), 50)\n",
    "    if alpha is None:\n",
    "        alpha = (1-0.02*math.log10(ndata), 1-0.001*math.log10(ndata))#).to(device)\n",
    "    if bounds is not None:\n",
    "        assert len(bounds) == ndim\n",
    "        for i in range(ndim):\n",
    "            assert len(bounds[i]) == 2\n",
    "    if edge_bins is None:\n",
    "        edge_bins = max(int(math.log10(ndata))-1, 0)\n",
    "    if batchsize is None:\n",
    "        batchsize = len(data_train)\n",
    "    if not patch:\n",
    "        if K is None:\n",
    "            if ndim <= 8 or ndata / float(ndim) < 20:\n",
    "                K = ndim\n",
    "            else:\n",
    "                K = 8\n",
    "        if ndata_A is None:\n",
    "            ndata_A = min(len(data_train), int(math.log10(ndim)*1e5))\n",
    "        if MSWD_max_iter is None:\n",
    "            MSWD_max_iter = min(round(ndata) // ndim, 200)\n",
    "    else:\n",
    "        assert shape[0] > 4 and shape[1] > 4\n",
    "        K0 = K\n",
    "        ndata_A0 = ndata_A\n",
    "        MSWD_max_iter0 = MSWD_max_iter\n",
    "\n",
    "        \n",
    "    best_accuracy = 0\n",
    "    #device\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() and not nocuda else \"cpu\")\n",
    "\n",
    "    \n",
    "    ### taking out logit transform and replacing log jacobian with zeros \n",
    "    logj_train    = torch.zeros(data_train.shape[0],device=device)\n",
    "    logj_valid    = torch.zeros(data_valid.shape[0],device=device)\n",
    "\n",
    "    #define the model\n",
    "    if model is None:\n",
    "        model = ConditionalSINF(ndim=ndim,n_class=nclass).requires_grad_(False).to(device)\n",
    "        if data_valid is not None:\n",
    "            best_logp_valid = -1e10\n",
    "            best_Nlayer     = 0\n",
    "            wait            = 0\n",
    "            maxwait         = 5 \n",
    "#     else:\n",
    "#         t = time.time()\n",
    "#         data_train, logj_train = conditional_transform_batch_model(model, data_train, batchsize, logj=logj_train, start=0, end=None, nocuda=nocuda)\n",
    "#         logp_train             = (torch.mean(logj_train) - ndim/2*torch.log(torch.tensor(2*math.pi)) - torch.mean(torch.sum(data_train**2,  dim=1)/2)).item()\n",
    "        \n",
    "#         if data_valid is not None:\n",
    "#             data_valid, logj_valid = conditional_transform_batch_model(model, data_valid, batchsize, logj=logj_valid, start=0, end=None, nocuda=nocuda)\n",
    "#             logp_valid             = (torch.mean(logj_valid) - ndim/2*torch.log(torch.tensor(2*math.pi)) - torch.mean(torch.sum(data_valid**2,  dim=1)/2)).item()\n",
    "#             print ('Initial logp:', logp_train, logp_valid, 'time:', time.time()-t, 'iteration:', len(model.layer))\n",
    "#         else:\n",
    "#             print ('Initial logp:', logp_train, 'time:', time.time()-t, 'iteration:', len(model.layer))\n",
    "\n",
    "    \n",
    "    #whiten\n",
    "    if Whiten:\n",
    "        layer = whiten(ndim_data=ndim, scale=True, ndim_latent=ndim).requires_grad_(False).to(device)\n",
    "        layer.fit(data_train)\n",
    "\n",
    "        data_train, logj_train0 = layer(data_train)\n",
    "        logj_train += logj_train0\n",
    "\n",
    "        logp_train = (torch.mean(logj_train) - ndim/2*torch.log(torch.tensor(2*math.pi)) - torch.mean(torch.sum(data_train**2,  dim=1)/2)).item()\n",
    "        \n",
    "        if data_valid is not None:\n",
    "            data_valid, logj_valid0 = layer(data_valid)\n",
    "            logj_valid += logj_valid0\n",
    "            logp_valid = (torch.mean(logj_valid) - ndim/2*torch.log(torch.tensor(2*math.pi)) - torch.mean(torch.sum(data_valid**2,  dim=1)/2)).item()\n",
    "            if logp_valid > best_logp_valid:\n",
    "                best_logp_valid = logp_valid\n",
    "                best_Nlayer = len(model.layer)\n",
    "\n",
    "        model.add_layer(layer)\n",
    "        if verbose:\n",
    "            if data_valid is not None:\n",
    "                print('After whiten logp:', logp_train, logp_valid)\n",
    "            else:\n",
    "                print('After whiten logp:', logp_train)\n",
    "\n",
    "                \n",
    "                \n",
    "    data_train    = data_train[None,:].repeat_interleave(nclass, axis=0)\n",
    "    data_valid    = data_valid[None,:].repeat_interleave(nclass, axis=0)\n",
    "\n",
    "    logj_train    = logj_train[None,:].repeat_interleave(nclass, axis=0)\n",
    "    logj_valid    = logj_valid[None,:].repeat_interleave(nclass, axis=0)\n",
    "\n",
    "    #GIS iterations\n",
    "    \n",
    "    ii=0\n",
    "    while True:\n",
    "        t = time.time()\n",
    "\n",
    "        if NBfirstlayer:\n",
    "            layer = ConditionalSlicedTransport_discrete(ndim=ndim, n_class = nclass, K=ndim, M=M).requires_grad_(False).to(device)\n",
    "        else:\n",
    "            layer = ConditionalSlicedTransport_discrete(ndim=ndim, n_class = nclass, K=K, M=M).requires_grad_(False).to(device)\n",
    "\n",
    "        #fit the layer\n",
    "        if NBfirstlayer:\n",
    "            layer.A[:] = torch.eye(ndim).to(device)\n",
    "            NBfirstlayer = False\n",
    "        elif ndim > 1:\n",
    "            layer.fit_A(data_train[label_train, torch.arange(data_train.shape[1]).to(device)], MSWD_max_iter=MSWD_max_iter, verbose=verbose)\n",
    "\n",
    "        layer.fit_spline(data_train[label_train, torch.arange(data_train.shape[1]).to(device)], label_train, edge_bins=edge_bins, derivclip=1, alpha=alpha, KDE=KDE, b_factor=b_factor, verbose=False)\n",
    "        \n",
    "        for label in range(nclass):\n",
    "            data_train[label], logj_train1 = layer(data_train[label], torch.ones(data_train.shape[1], dtype=torch.int, device=data_train.device)*label)\n",
    "            logj_train[label] = logj_train[label] + logj_train1\n",
    "\n",
    "            data_valid[label], logj_valid1 = layer(data_valid[label], torch.ones(data_valid.shape[1], dtype=torch.int, device=data_valid.device)*label)\n",
    "            logj_valid[label] = logj_valid[label] + logj_valid1\n",
    "\n",
    "        model.add_layer(layer)\n",
    "\n",
    "\n",
    "#     for label in range(n_class):\n",
    "#         data_train[label], logj_train1 = layer(data_train[label], torch.ones(data_train.shape[1], dtype=torch.int, device=data_train.device)*label)\n",
    "#         logp_train[label]              = logj_train - torch.sum(data_train**2,  dim=1)/2- ndim/2*torch.log(torch.tensor(2*math.pi))\n",
    "\n",
    "#         data_valid[label], logj_valid1 = layer(data_validate[label], torch.ones(data_validate.shape[1], dtype=torch.int, device=data_validate.device)*label)\n",
    "#         logp_valid[label]              = logj_valid\n",
    "\n",
    "#         data_test[label], logj_test1 = layer(data_test[label], torch.ones(data_test.shape[1], dtype=torch.int, device=data_test.device)*label)\n",
    "#         logj_test[label]             = logj_test[label] + logj_test1\n",
    "        \n",
    "        logp_train = (torch.mean(logj_train[label_train, torch.arange(data_train.shape[1]).to(device)]) - ndim/2*torch.log(torch.tensor(2*math.pi)) - torch.mean(torch.sum(data_train[label_train, torch.arange(data_train.shape[1]).to(device)]**2,  dim=1)/2)).item()\n",
    "        logp_valid = (torch.mean(logj_valid[label_valid, torch.arange(data_valid.shape[1]).to(device)]) - ndim/2*torch.log(torch.tensor(2*math.pi)) - torch.mean(torch.sum(data_valid[label_valid, torch.arange(data_valid.shape[1]).to(device)]**2,  dim=1)/2)).item()\n",
    "\n",
    "        \n",
    "#         logp_valid_label = torch.ones([nclass,label_valid.shape[0]], device=data_valid.device)\n",
    "#         if data_valid is not None:\n",
    "#             for mm in range(nclass):\n",
    "#                 data_valid, logj_valid = conditional_transform_batch_layer(layer, data_valid, torch.ones(label_valid.shape, dtype=torch.int, device=data_valid.device)*mm, batchsize, logj=logj_valid, direction='forward', nocuda=nocuda)\n",
    "#                 logp_valid = (logj_valid - torch.sum(data_valid**2,  dim=1)/2)- ndim/2*torch.log(torch.tensor(2*math.pi))\n",
    "#                 logp_valid_label[mm] = logp_valid\n",
    "#             predict_label_valid = torch.argmax(logp_valid_label,dim=0)\n",
    "#             accuracy = torch.sum(predict_label_valid==label_valid).item() / len(label_valid)\n",
    "#             print('valid accuracy: ', torch.sum(predict_label_valid==label_valid).item() / len(label_valid))\n",
    "    \n",
    "#             data_valid, logj_valid = conditional_transform_batch_layer(layer, data_valid, label_valid, batchsize, logj=logj_valid, direction='forward', nocuda=nocuda)\n",
    "#             logp_valid = (torch.mean(logj_valid) - ndim/2*torch.log(torch.tensor(2*math.pi)) - torch.mean(torch.sum(data_valid**2,  dim=1)/2)).item()\n",
    "\n",
    "            \n",
    "        if logp_valid > best_logp_valid:\n",
    "            best_logp_valid = logp_valid\n",
    "            best_Nlayer = len(model.layer)\n",
    "            wait = 0\n",
    "        else:\n",
    "            wait += 1\n",
    "        if wait == maxwait:\n",
    "            model.layer = model.layer[:best_Nlayer]\n",
    "            break\n",
    "        if ii>max_iter:\n",
    "            break\n",
    "\n",
    "#         if  accuracy > best_accuracy:\n",
    "#             best_accuracy = accuracy\n",
    "#             best_Nlayer_acc = len(model.layer)\n",
    "#             wait_acc = 0\n",
    "#         else:\n",
    "#             wait_acc += 1\n",
    "#         if wait_acc == maxwait:\n",
    "#             model.layer = model.layer[:best_Nlayer_acc]\n",
    "#             break\n",
    "\n",
    "        if verbose:\n",
    "            if data_valid is not None: \n",
    "                print ('logp:', logp_train, logp_valid, 'time:', time.time()-t, 'iteration:', len(model.layer), 'best:', best_Nlayer)\n",
    "            else:\n",
    "                print ('logp:', logp_train, 'time:', time.time()-t, 'iteration:', len(model.layer))\n",
    "\n",
    "        if iteration is not None and len(model.layer) >= iteration:\n",
    "            if data_valid is not None:\n",
    "                model.layer = model.layer[:best_Nlayer]\n",
    "            break\n",
    "        ii+=1\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "87a5a074-cad9-4b30-a6a8-9856bf34f973",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 2\n",
      "After whiten logp: -0.38007354736328125 -0.3476181626319885\n",
      "Fit A: Time: 0.008809804916381836 Wasserstein Distance: [0.2300717979669571, 0.21073226630687714]\n",
      "logp: -0.371601939201355 -0.33950966596603394 time: 0.0233304500579834 iteration: 2 best: 2\n",
      "Fit A: Time: 0.0094757080078125 Wasserstein Distance: [0.2268887460231781, 0.20840848982334137]\n",
      "logp: -0.36339259147644043 -0.3316289186477661 time: 0.024180889129638672 iteration: 3 best: 3\n",
      "Fit A: Time: 0.012303352355957031 Wasserstein Distance: [0.17150181531906128, 0.11756326258182526]\n",
      "logp: -0.3597933053970337 -0.3324967622756958 time: 0.026059627532958984 iteration: 4 best: 3\n",
      "Fit A: Time: 0.012814044952392578 Wasserstein Distance: [0.2237948328256607, 0.2055627852678299]\n",
      "logp: -0.3518783450126648 -0.32481032609939575 time: 0.026737213134765625 iteration: 5 best: 5\n",
      "Fit A: Time: 0.012503385543823242 Wasserstein Distance: [0.22085440158843994, 0.20337000489234924]\n",
      "logp: -0.34420955181121826 -0.31734395027160645 time: 0.026448965072631836 iteration: 6 best: 6\n",
      "Fit A: Time: 0.013277769088745117 Wasserstein Distance: [0.217999666929245, 0.20123516023159027]\n",
      "logp: -0.3367730379104614 -0.3100889325141907 time: 0.02791762351989746 iteration: 7 best: 7\n",
      "Fit A: Time: 0.007448911666870117 Wasserstein Distance: [0.2151162475347519, 0.19925633072853088]\n",
      "logp: -0.3295603394508362 -0.3030461072921753 time: 0.02164459228515625 iteration: 8 best: 8\n",
      "Fit A: Time: 0.010619640350341797 Wasserstein Distance: [0.21249498426914215, 0.1971304714679718]\n",
      "logp: -0.32256805896759033 -0.2961984872817993 time: 0.026145219802856445 iteration: 9 best: 9\n",
      "Fit A: Time: 0.008310317993164062 Wasserstein Distance: [0.16939476132392883, 0.11287150532007217]\n",
      "logp: -0.31912094354629517 -0.2974585294723511 time: 0.022470951080322266 iteration: 10 best: 9\n",
      "Fit A: Time: 0.014325380325317383 Wasserstein Distance: [0.20907504856586456, 0.19540375471115112]\n",
      "logp: -0.31233394145965576 -0.29083752632141113 time: 0.02804088592529297 iteration: 11 best: 11\n",
      "Fit A: Time: 0.008882761001586914 Wasserstein Distance: [0.206030011177063, 0.1938147246837616]\n",
      "logp: -0.30573368072509766 -0.284424364566803 time: 0.022963762283325195 iteration: 12 best: 12\n",
      "Fit A: Time: 0.014429807662963867 Wasserstein Distance: [0.20320262014865875, 0.19208398461341858]\n",
      "logp: -0.29932665824890137 -0.27820438146591187 time: 0.028363943099975586 iteration: 13 best: 13\n",
      "Fit A: Time: 0.020864486694335938 Wasserstein Distance: [0.20053718984127045, 0.19025962054729462]\n",
      "logp: -0.2931098937988281 -0.27216118574142456 time: 0.03615927696228027 iteration: 14 best: 14\n",
      "Fit A: Time: 0.013216733932495117 Wasserstein Distance: [0.19777363538742065, 0.1886429786682129]\n",
      "logp: -0.28706836700439453 -0.26630181074142456 time: 0.02770709991455078 iteration: 15 best: 15\n",
      "Fit A: Time: 0.011530876159667969 Wasserstein Distance: [0.19654221832752228, 0.1852719783782959]\n",
      "logp: -0.281280517578125 -0.2605447769165039 time: 0.025532007217407227 iteration: 16 best: 16\n",
      "Fit A: Time: 0.044706106185913086 Wasserstein Distance: [0.19268786907196045, 0.18524156510829926]\n",
      "logp: -0.275587260723114 -0.2550160884857178 time: 0.05883526802062988 iteration: 17 best: 17\n",
      "Fit A: Time: 0.018128633499145508 Wasserstein Distance: [0.19030047953128815, 0.18350225687026978]\n",
      "logp: -0.2700647711753845 -0.24963796138763428 time: 0.03226470947265625 iteration: 18 best: 18\n",
      "Fit A: Time: 0.01577305793762207 Wasserstein Distance: [0.18792328238487244, 0.18184636533260345]\n",
      "logp: -0.2647063732147217 -0.244412362575531 time: 0.02990126609802246 iteration: 19 best: 19\n",
      "Fit A: Time: 0.015493154525756836 Wasserstein Distance: [0.18546800315380096, 0.18032030761241913]\n",
      "logp: -0.2595021724700928 -0.23933833837509155 time: 0.029740571975708008 iteration: 20 best: 20\n",
      "Fit A: Time: 0.014807701110839844 Wasserstein Distance: [0.18310560286045074, 0.17873136699199677]\n",
      "logp: -0.25445306301116943 -0.23440611362457275 time: 0.02868175506591797 iteration: 21 best: 21\n",
      "Fit A: Time: 0.010172367095947266 Wasserstein Distance: [0.18037711083889008, 0.1775776892900467]\n",
      "logp: -0.2495288848876953 -0.22963815927505493 time: 0.024109363555908203 iteration: 22 best: 22\n",
      "Fit A: Time: 0.027020692825317383 Wasserstein Distance: [0.17826633155345917, 0.17589637637138367]\n",
      "logp: -0.24476152658462524 -0.22499024868011475 time: 0.04109764099121094 iteration: 23 best: 23\n",
      "Fit A: Time: 0.017165184020996094 Wasserstein Distance: [0.1758728176355362, 0.17456211149692535]\n",
      "logp: -0.24012362957000732 -0.22048026323318481 time: 0.03205299377441406 iteration: 24 best: 24\n",
      "Fit A: Time: 0.014170408248901367 Wasserstein Distance: [0.1736421287059784, 0.1731329709291458]\n",
      "logp: -0.23561781644821167 -0.21609705686569214 time: 0.028609037399291992 iteration: 25 best: 25\n",
      "Fit A: Time: 0.01006317138671875 Wasserstein Distance: [0.17185978591442108, 0.17133258283138275]\n",
      "logp: -0.23123741149902344 -0.21184462308883667 time: 0.023925304412841797 iteration: 26 best: 26\n",
      "Fit A: Time: 0.009252786636352539 Wasserstein Distance: [0.17054753005504608, 0.16913509368896484]\n",
      "logp: -0.2269774079322815 -0.20771539211273193 time: 0.022681474685668945 iteration: 27 best: 27\n",
      "Fit A: Time: 0.009224414825439453 Wasserstein Distance: [0.16322773694992065, 0.12167510390281677]\n",
      "logp: -0.2235129475593567 -0.20929783582687378 time: 0.02296137809753418 iteration: 28 best: 27\n",
      "Fit A: Time: 0.014237642288208008 Wasserstein Distance: [0.16926366090774536, 0.16674242913722992]\n",
      "logp: -0.2193724513053894 -0.2053050398826599 time: 0.028023481369018555 iteration: 29 best: 29\n",
      "Fit A: Time: 0.015186786651611328 Wasserstein Distance: [0.1674399971961975, 0.16425958275794983]\n",
      "logp: -0.2154083251953125 -0.20128804445266724 time: 0.029503583908081055 iteration: 30 best: 30\n",
      "Fit A: Time: 0.0066013336181640625 Wasserstein Distance: [0.16166964173316956, 0.13512958586215973]\n",
      "logp: -0.21195894479751587 -0.19991689920425415 time: 0.021508455276489258 iteration: 31 best: 31\n",
      "Fit A: Time: 0.01216888427734375 Wasserstein Distance: [0.16622288525104523, 0.16173213720321655]\n",
      "logp: -0.20806485414505005 -0.19621479511260986 time: 0.027144193649291992 iteration: 32 best: 32\n",
      "Fit A: Time: 0.009958028793334961 Wasserstein Distance: [0.15934281051158905, 0.11905987560749054]\n",
      "logp: -0.20481932163238525 -0.19809991121292114 time: 0.024001598358154297 iteration: 33 best: 32\n",
      "Fit A: Time: 0.01127314567565918 Wasserstein Distance: [0.16502876579761505, 0.15944121778011322]\n",
      "logp: -0.20102709531784058 -0.19464093446731567 time: 0.025736093521118164 iteration: 34 best: 34\n",
      "Fit A: Time: 0.01644110679626465 Wasserstein Distance: [0.1638244390487671, 0.15744201838970184]\n",
      "logp: -0.1973441243171692 -0.19127154350280762 time: 0.030306339263916016 iteration: 35 best: 35\n",
      "Fit A: Time: 0.014731884002685547 Wasserstein Distance: [0.1626802682876587, 0.1554214358329773]\n",
      "logp: -0.19388526678085327 -0.18802177906036377 time: 0.02934408187866211 iteration: 36 best: 36\n",
      "Fit A: Time: 0.013923168182373047 Wasserstein Distance: [0.1567087322473526, 0.11770409345626831]\n",
      "logp: -0.19076251983642578 -0.18996262550354004 time: 0.028158187866210938 iteration: 37 best: 36\n",
      "Fit A: Time: 0.016930103302001953 Wasserstein Distance: [0.16133254766464233, 0.15316596627235413]\n",
      "logp: -0.18727147579193115 -0.18693846464157104 time: 0.031152725219726562 iteration: 38 best: 38\n",
      "Fit A: Time: 0.017495393753051758 Wasserstein Distance: [0.16026638448238373, 0.1511593461036682]\n",
      "logp: -0.18387120962142944 -0.18401139974594116 time: 0.03162550926208496 iteration: 39 best: 39\n",
      "Fit A: Time: 0.009504318237304688 Wasserstein Distance: [0.15417779982089996, 0.15299350023269653]\n",
      "logp: -0.18083077669143677 -0.18072503805160522 time: 0.023362159729003906 iteration: 40 best: 40\n",
      "Fit A: Time: 0.013715505599975586 Wasserstein Distance: [0.15575966238975525, 0.14166072010993958]\n",
      "logp: -0.1772916316986084 -0.17893582582473755 time: 0.02769637107849121 iteration: 41 best: 41\n",
      "Fit A: Time: 0.014340639114379883 Wasserstein Distance: [0.12683038413524628, 0.10411584377288818]\n",
      "logp: -0.17544430494308472 -0.1794763207435608 time: 0.028940439224243164 iteration: 42 best: 41\n",
      "Fit A: Time: 0.011901378631591797 Wasserstein Distance: [0.1535658836364746, 0.13923662900924683]\n",
      "logp: -0.17204958200454712 -0.17778652906417847 time: 0.025590896606445312 iteration: 43 best: 43\n",
      "Fit A: Time: 0.011184930801391602 Wasserstein Distance: [0.15309233963489532, 0.11345028877258301]\n",
      "logp: -0.16915768384933472 -0.17981421947479248 time: 0.02469611167907715 iteration: 44 best: 43\n",
      "Fit A: Time: 0.014481544494628906 Wasserstein Distance: [0.1521097719669342, 0.12316107004880905]\n",
      "logp: -0.16627848148345947 -0.17918843030929565 time: 0.02809429168701172 iteration: 45 best: 43\n",
      "Fit A: Time: 0.011021137237548828 Wasserstein Distance: [0.14996100962162018, 0.14664989709854126]\n",
      "logp: -0.16351526975631714 -0.17620015144348145 time: 0.025085926055908203 iteration: 46 best: 46\n",
      "Fit A: Time: 0.017983436584472656 Wasserstein Distance: [0.1536719799041748, 0.14096930623054504]\n",
      "logp: -0.1606818437576294 -0.17373889684677124 time: 0.03223013877868652 iteration: 47 best: 47\n",
      "Fit A: Time: 0.00907444953918457 Wasserstein Distance: [0.1496165543794632, 0.13428787887096405]\n",
      "logp: -0.1575382947921753 -0.1722700595855713 time: 0.022761106491088867 iteration: 48 best: 48\n",
      "Fit A: Time: 0.012480020523071289 Wasserstein Distance: [0.1484401971101761, 0.13254588842391968]\n",
      "logp: -0.15448129177093506 -0.17088454961776733 time: 0.02624034881591797 iteration: 49 best: 49\n",
      "Fit A: Time: 0.005892515182495117 Wasserstein Distance: [0.15064431726932526, 0.13642354309558868]\n",
      "logp: -0.15178686380386353 -0.1686093807220459 time: 0.019750118255615234 iteration: 50 best: 50\n",
      "Fit A: Time: 0.018519878387451172 Wasserstein Distance: [0.14730191230773926, 0.112442746758461]\n",
      "logp: -0.149053156375885 -0.17066049575805664 time: 0.03500223159790039 iteration: 51 best: 50\n",
      "Fit A: Time: 0.018123149871826172 Wasserstein Distance: [0.14499783515930176, 0.11128534376621246]\n",
      "logp: -0.1464279294013977 -0.1727345585823059 time: 0.038224220275878906 iteration: 52 best: 50\n",
      "Fit A: Time: 0.01201176643371582 Wasserstein Distance: [0.14624746143817902, 0.12808333337306976]\n",
      "logp: -0.1435500979423523 -0.171464741230011 time: 0.02607131004333496 iteration: 53 best: 50\n",
      "Fit A: Time: 0.01033473014831543 Wasserstein Distance: [0.14231403172016144, 0.11022094637155533]\n",
      "logp: -0.1410263180732727 -0.17355221509933472 time: 0.025289058685302734 iteration: 54 best: 50\n",
      "Fit A: Time: 0.007244110107421875 Wasserstein Distance: [0.13567210733890533, 0.11995788663625717]\n"
     ]
    }
   ],
   "source": [
    "model = train_ConditionalGIS(train_x, train_y,valid_x, valid_y, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c1cc5e4-4d1a-4524-a2ce-3a2acd85d863",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
